{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishanshehadeh/opt/anaconda3/envs/WTP/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv('PINECONE_API_KEY')\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", streaming = True)\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
    "\n",
    "MAX_RETRIEVAL_ATTEMPTS = 4\n",
    "\n",
    "class ExtState(MessagesState):\n",
    "    retrieval_attempts: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up vector store\n",
    "vs = PineconeVectorStore(\n",
    "    index_name = 'langchain-index',\n",
    "    embedding = embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "chat_history = [HumanMessage(content=\"Tell me something about Joe Biden's foreign policy in Ukraine\"), HumanMessage(content=\"How has that impacted Russia?\")]\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads chats up to x tokens for context for each inquiry, \n",
    "def history(state):\n",
    "    # Prompt for history contextualizing\n",
    "    print(\"---ADDING HISTORY---\")\n",
    "    print(f\"{chat_history}\")\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Given a chat history and the latest user question \n",
    "        which might reference context in the chat history,\n",
    "        formulate a standalone question which can be understood \n",
    "        without the chat history. Do NOT answer the question,\n",
    "        just reformulate it if needed and otherwise return it as is.\n",
    "        Here is the chat history: \\n\\n {chat_history} \\n\\n\n",
    "        Here is the latest user question: {question} \\n\\n\n",
    "        \"\"\",\n",
    "        input_variables=[\"chat_history\", \"query\"],\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    response = chain.invoke({\"chat_history\": chat_history, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "# Node 1 - Retriever Tool \n",
    "# possible switch to vector_store.as_retriever\n",
    "@tool(response_format =\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    \n",
    "    print(\"---RETRIEVE 6 RELEVANT DOCS---\")\n",
    "    retrieved_docs = vs.similarity_search(query, k=6)\n",
    "    \n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return \"\", \"\"\n",
    "\n",
    "# Node 2 - grader\n",
    "def grade_documents(state) -> Literal[\"rewrite\", \"generate\"]:\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state[\"retrieval_attempts\"] += 1\n",
    "    print(type(state))\n",
    "    print(f\"Retrieval attempts: {state.get(\"retrieval_attempts\")}\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    model = llm\n",
    "    \n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt for binary grading\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[1].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "    \n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return state, \"generate\"\n",
    "    # max attempts\n",
    "    elif score == \"no\" and state[\"retrieval_attempts\"] >= MAX_RETRIEVAL_ATTEMPTS:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT & MAX RETRIEVAL ATTEMPTS REACHED---\")\n",
    "        return state, \"direct_response\"\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return state, \"rewrite\"\n",
    "\n",
    "# Node 2 - Query or Respond agent --> decides whether to retrieve or respond\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def agent(state):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    #print(f\"Test original message: {state[\"messages\"][1]}\")\n",
    "    print(\"---CALL AGENT---\")\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def rewrite(state):\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[1].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate a new question that is specific and deatiled to help with document retrieval. \n",
    "    Focus on key terms and concepts that might appear in relevant documents: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Node 3 - Generate Answer\n",
    "def generate(state):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    print(\"---GENERATE ANSWER---\")\n",
    "    # Get generated ToolMessages\n",
    "    messages = state[\"messages\"]\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt --> adds former messages from graph\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    question = messages[1].content \n",
    "    \n",
    "    prompt = PromptTemplate(template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise. \\n \n",
    "        Here is the original question: \\n\\n {question} \\n\\n\n",
    "        Here is the context: {context} \\n\\n\n",
    "        Give a one line summary of the context metadata used at the end of your answer.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        )\n",
    "    gen_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = gen_chain.invoke({\"context\": docs_content, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def direct_response(state):\n",
    "    \"\"\"Generate a highly constrained response when not using tools.\"\"\"\n",
    "    print(\"---DIRECT RESPONSE---\")\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a focused assistant that only provides brief, targeted responses.\n",
    "        Provide a two sentence response that politely explains you can only answer questions about the specific content in our knowledge base which provides information about American policy .\n",
    "        Do not provide any other information or engage in general conversation.\n",
    "        \n",
    "        User question: {question}\n",
    "        \n",
    "        One sentence response:\"\"\",\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = ToolNode([retrieve])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(ExtState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"history\", history)\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "workflow.add_node(\"retrieve\", tools)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\"generate\", generate) # generate answer\n",
    "workflow.add_node(\"direct_response\", direct_response) # direct response for irrelevant question\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"history\")\n",
    "workflow.add_edge(\"history\", \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: \"direct_response\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    lambda x: grade_documents(x)[1],\n",
    "    {\n",
    "        \"rewrite\": \"rewrite\",\n",
    "        \"generate\": \"generate\",\n",
    "        \"direct_response\": \"direct_response\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"direct_response\", END)\n",
    "\n",
    "\n",
    "# Compile\n",
    "#memory = MemorySaver()\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ADDING HISTORY---\n",
      "[HumanMessage(content=\"Tell me something about Joe Biden's foreign policy in Ukraine\"), HumanMessage(content='How has that impacted Russia?')]\n",
      "---CALL AGENT---\n",
      "---RETRIEVE 6 RELEVANT DOCS---\n",
      "---CHECK RELEVANCE---\n",
      "<class 'dict'>\n",
      "Retrieval attempts: 1\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n",
      "---RETRIEVE 6 RELEVANT DOCS------RETRIEVE 6 RELEVANT DOCS---\n",
      "\n",
      "---RETRIEVE 6 RELEVANT DOCS---\n",
      "---CHECK RELEVANCE---\n",
      "<class 'dict'>\n",
      "Retrieval attempts: 1\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n",
      "---RETRIEVE 6 RELEVANT DOCS------RETRIEVE 6 RELEVANT DOCS---\n",
      "\n",
      "---RETRIEVE 6 RELEVANT DOCS---\n",
      "---CHECK RELEVANCE---\n",
      "<class 'dict'>\n",
      "Retrieval attempts: 1\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Tell me more about the first thing I asked about\")], \"retrieval_attempts\": 0}\n",
    "for msg, metadata in graph.stream(inputs, stream_mode=\"messages\"):\n",
    "    #print(f\"Node: {metadata['langgraph_node']}\")\n",
    "    if (\n",
    "        msg.content\n",
    "        #and not isinstance(msg, HumanMessage)\n",
    "        and metadata[\"langgraph_node\"] == \"generate\"\n",
    "    ):\n",
    "        print(msg.content, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Tell me about the meaning of life\")], \"retrieval_attempts\": 0}\n",
    "for msg, metadata in graph.stream(inputs, stream_mode=\"messages\"):\n",
    "    #print(f\"Node: {metadata['langgraph_node']}\")\n",
    "    if (\n",
    "        msg.content\n",
    "        #and not isinstance(msg, HumanMessage)\n",
    "        and metadata[\"langgraph_node\"] == \"generate\" or metadata[\"langgraph_node\"] == \"direct_response\"\n",
    "    ):\n",
    "        print(msg.content, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WTP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
